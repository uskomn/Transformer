{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cd99759-6edf-44cf-94bc-b8e23fb060f5",
   "metadata": {},
   "source": [
    "# 动手实现 Transformer\n",
    "\n",
    "> ![模型架构图](../assets/20241023202539.png)\n",
    ">\n",
    "> 《[Transformer 论文精读](https://github.com/Hoper-J/AI-Guide-and-Demos-zh_CN/blob/master/PaperNotes/Transformer%20论文精读.md)》\n",
    "\n",
    "当前代码文件将从零开始构建 Transformer（PyTorch 版），建议结合文章进行理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900cd475-50fe-4c40-89f4-9f1992ad6641",
   "metadata": {},
   "source": [
    "# 导入库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32f2efe0-bec1-4b47-b595-7d187e184573",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d2d6714-ba07-45f7-822f-89004f914787",
   "metadata": {},
   "source": [
    "# 子模块"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e878aefc-3014-4f1d-b060-1881d555d2ff",
   "metadata": {},
   "source": [
    "## 缩放点积注意力机制\n",
    "\n",
    "> ![缩放点积注意力机制](../assets/image-20241024010439683.png)\n",
    "\n",
    "给定查询矩阵 $Q$、键矩阵 $K$ 和值矩阵 $V$, 其注意力输出的数学表达式如下：\n",
    "\n",
    "$$\n",
    "\\text{Attention}(Q, K, V) = \\text{Softmax}\\left(\\frac{Q K^\\top}{\\sqrt{d_k}}\\right) V\n",
    "$$\n",
    "\n",
    "- **$Q$（Query）**: 用于查询的向量矩阵。\n",
    "- **$K$（Key）**: 表示键的向量矩阵，用于与查询匹配。\n",
    "- **$V$（Value）**: 值矩阵，注意力权重最终会作用在该矩阵上。\n",
    "- **$d_k$**: 键或查询向量的维度。\n",
    "\n",
    "> 理解 Q、K、V 的关键在于代码，它们实际上是通过线性变换从输入序列生成的，“故事”的延伸更多是锦上添花。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579ac151-68dc-4900-b4b9-a0b85de0fa0c",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d06f5849-0ca2-4813-a980-0601d6e4a811",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    缩放点积注意力计算。\n",
    "    \n",
    "    参数:\n",
    "        Q: 查询矩阵 (batch_size, seq_len_q, embed_size)\n",
    "        K: 键矩阵 (batch_size, seq_len_k, embed_size)\n",
    "        V: 值矩阵 (batch_size, seq_len_v, embed_size)\n",
    "        mask: 掩码矩阵，用于屏蔽不应该关注的位置 (可选)\n",
    "\n",
    "    返回:\n",
    "        output: 注意力加权后的输出矩阵\n",
    "        attention_weights: 注意力权重矩阵\n",
    "    \"\"\"\n",
    "    embed_size = Q.size(-1)  # embed_size\n",
    "    \n",
    "    # 计算点积并进行缩放\n",
    "    scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(embed_size)\n",
    "\n",
    "    # 如果提供了掩码矩阵，则将掩码对应位置的分数设为 -inf\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "\n",
    "    # 对缩放后的分数应用 Softmax 函数，得到注意力权重\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "\n",
    "    # 加权求和，计算输出\n",
    "    output = torch.matmul(attention_weights, V)\n",
    "    \n",
    "    return output, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2119dce1-6068-431f-9ce0-4ce79c1f099f",
   "metadata": {},
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d6a396f-a49e-4e46-b596-2cb60625d525",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "掩码矩阵 (下三角):\n",
      "tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "\n",
      "注意力权重矩阵:\n",
      "tensor([[[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4416, 0.5584, 0.0000],\n",
      "          [0.1342, 0.2052, 0.6607]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.4896, 0.5104, 0.0000],\n",
      "          [0.3431, 0.1530, 0.5039]]],\n",
      "\n",
      "\n",
      "        [[[1.0000, 0.0000, 0.0000],\n",
      "          [0.4945, 0.5055, 0.0000],\n",
      "          [0.3274, 0.2313, 0.4413]],\n",
      "\n",
      "         [[1.0000, 0.0000, 0.0000],\n",
      "          [0.1885, 0.8115, 0.0000],\n",
      "          [0.3620, 0.2329, 0.4051]]]])\n"
     ]
    }
   ],
   "source": [
    "# 示例参数\n",
    "batch_size = 2\n",
    "num_heads = 2\n",
    "seq_len_q = 3  # 查询序列长度\n",
    "seq_len_k = 3  # 键序列长度\n",
    "head_dim = 4\n",
    "\n",
    "# 模拟查询矩阵 Q 和键值矩阵 K, V\n",
    "Q = torch.randn(batch_size, num_heads, seq_len_q, head_dim)\n",
    "K = torch.randn(batch_size, num_heads, seq_len_k, head_dim)\n",
    "V = torch.randn(batch_size, num_heads, seq_len_k, head_dim)\n",
    "\n",
    "# 生成下三角掩码矩阵 (1, 1, seq_len_q, seq_len_k)，通过广播应用到所有头\n",
    "mask = torch.tril(torch.ones(seq_len_q, seq_len_k)).unsqueeze(0).unsqueeze(0)  # mask.shape (seq_len_q, seq_len_k) -> (1, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "# 执行缩放点积注意力，并应用下三角掩码\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "# 打印结果\n",
    "print(\"掩码矩阵 (下三角):\")\n",
    "print(mask[0, 0])\n",
    "\n",
    "print(\"\\n注意力权重矩阵:\")\n",
    "print(attn_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c944951c-f9f4-478d-9286-035c66fe01d1",
   "metadata": {},
   "source": [
    "## 多头注意力机制（Multi-Head Attention）\n",
    "\n",
    "多头注意力机制在 Transformer 中发挥着与卷积神经网络（CNN）中的**卷积核**（Kernel）类似的作用。CNN 使用多个不同的卷积核在空间域上捕捉不同的局部特征，而 Transformer 的多头注意力通过**多个头**（Head）并行地关注输入数据在不同维度上的依赖关系。\n",
    "\n",
    "### 数学表达\n",
    "\n",
    "假设我们有 $h$ 个头，每个头拥有独立的线性变换矩阵 $W_i^Q, W_i^K, W_i^V$（分别作用于查询、键和值的映射），每个头的计算如下：\n",
    "\n",
    "$$\n",
    "\\text{head}_i = \\text{Attention}(Q W_i^Q, K W_i^K, V W_i^V)\n",
    "$$\n",
    "\n",
    "这些头的输出将沿最后一维拼接（**Concat**），并通过线性变换矩阵 $W^O$ 映射回原始嵌入维度（`embed_size`）：\n",
    "\n",
    "$$\n",
    "\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\n",
    "$$\n",
    "\n",
    "- **$h$**：注意力头的数量。\n",
    "- **$W^O$**：拼接后所通过的线性变换矩阵，用于将多头的输出映射回原始维度。  \n",
    "\n",
    "> ![Encoder](../assets/image-20241027191251526.png)\n",
    ">\n",
    "> 映射回原始维度的主要目的是为了实现残差连接（Residual Connection），即：\n",
    ">\n",
    "> $x + \\text{SubLayer}(x)$\n",
    ">\n",
    "> 你将发现其他模块（如自注意力模块、多头注意力机制和前馈网络）的输出层大多都是一样的维度，这是因为只有当输入 $x$ 的形状与经过层变换后的输出 $\\text{SubLayer}(x)$ 的形状一致时，才能按预期的进行逐元素相加（element-wise addition），否则会导致张量维度不匹配，需要额外的变换操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe207d3f-5564-44b3-8134-39f659fbbac1",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46387b63-80c9-490b-bc6e-da9ca1dc7fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h):\n",
    "        \"\"\"\n",
    "        多头注意力机制：每个头单独定义线性层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入序列的嵌入维度。\n",
    "            h: 注意力头的数量。\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0, \"d_model 必须能被 h 整除。\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # “共享”的 Q, K, V 线性层\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # 输出线性层，将多头拼接后的输出映射回 d_model\n",
    "        self.fc_out = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, q, k, v, mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            q: 查询矩阵 (batch_size, seq_len_q, d_model)\n",
    "            k: 键矩阵 (batch_size, seq_len_k, d_model)\n",
    "            v: 值矩阵 (batch_size, seq_len_v, d_model)\n",
    "            mask: 掩码矩阵 (batch_size, 1, seq_len_q, seq_len_k)\n",
    "\n",
    "        返回:\n",
    "            out: 注意力加权后的输出\n",
    "            attention_weights: 注意力权重矩阵\n",
    "        \"\"\"\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # 获取查询和键值的序列长度\n",
    "        seq_len_q = q.size(1)\n",
    "        seq_len_k = k.size(1)\n",
    "\n",
    "        # 将线性变换后的“共享”矩阵拆分为多头，调整维度为 (batch_size, h, seq_len, d_k)\n",
    "        # d_k 就是每个注意力头的维度\n",
    "        Q = self.w_q(q).view(batch_size, seq_len_q, self.h, -1).transpose(1, 2)\n",
    "        K = self.w_k(k).view(batch_size, seq_len_k, self.h, -1).transpose(1, 2)\n",
    "        V = self.w_v(v).view(batch_size, seq_len_k, self.h, -1).transpose(1, 2)\n",
    "\n",
    "        # 执行缩放点积注意力\n",
    "        scaled_attention, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "\n",
    "        # 合并多头并还原为 (batch_size, seq_len_q, d_model)\n",
    "        concat_out = scaled_attention.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "\n",
    "        # 通过输出线性层\n",
    "        out = self.fc_out(concat_out)  # (batch_size, seq_len_q, d_model)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd5b1c2-9a32-43fb-acc3-0ce22f1038a3",
   "metadata": {},
   "source": [
    "## Position-wise Feed-Forward Networks（FFN）\n",
    "\n",
    "> ![Position-wise Feed-Forward Networks](../assets/image-20241028151143736.png)\n",
    "\n",
    "### 数学表达\n",
    "\n",
    "> ![FFN](../assets/image-20241028151815767.png)\n",
    "\n",
    "在编码器-解码器架构中，另一个看起来“大一点”的模块就是 Feed Forward，它在每个位置 $i$ 上的计算可以表示为：\n",
    "\n",
    "$$\n",
    "\\text{FFN}(x_i) = \\text{max}(0, x_i W_1 + b_1) W_2 + b_2\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $x_i \\in \\mathbb{R}^{d_{\\text{model}}}$ 表示第 $i$ 个位置的输入向量。 \n",
    "- $W_1 \\in \\mathbb{R}^{d_{\\text{model}} \\times d_{\\text{ff}}}$ 和 $W_2 \\in \\mathbb{R}^{d_{\\text{ff}} \\times d_{\\text{model}}}$ 是两个线性变换的权重矩阵。\n",
    "- $b_1 \\in \\mathbb{R}^{d_{\\text{ff}}}$ 和 $b_2 \\in \\mathbb{R}^{d_{\\text{model}}}$ 是对应的偏置向量。\n",
    "- $\\text{max}(0, \\cdot)$ 是 **ReLU 激活函数**，用于引入非线性。\n",
    "\n",
    "Position-wise 实际是线性层本身的一个特性，在线性层中，每个输入向量（对应于序列中的一个位置，比如一个词向量）都会通过相同的权重矩阵进行线性变换，这意味着每个位置的处理是相互独立的，逐元素这一点可以看成 kernal_size=1 的卷积核扫过一遍序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2531c9-3b1f-44eb-826b-934ad6f37384",
   "metadata": {},
   "source": [
    "#### 代码实现\n",
    "\n",
    "FFN 本质就是两个线性变换之间嵌入了一个 **ReLU** 激活函数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce31f71-b9dc-4370-a494-4778e4b935ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        位置前馈网络。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 输入和输出向量的维度\n",
    "            d_ff: FFN 隐藏层的维度，或者说中间层\n",
    "            dropout: 随机失活率（Dropout），即随机屏蔽部分神经元的输出，用于防止过拟合\n",
    "        \n",
    "        （实际上论文并没有确切地提到在这个模块使用 dropout，所以注释）\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)  # 第一个线性层\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)  # 第二个线性层\n",
    "        #self.dropout = nn.Dropout(dropout)   # Dropout 层\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 先经过第一个线性层和 ReLU，然后经过第二个线性层\n",
    "        return self.w_2(self.w_1(x).relu())  #self.w_2(self.dropout(self.w_1(x).relu()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede17cb-960b-4453-ae19-6047654cab24",
   "metadata": {},
   "source": [
    "## 残差连接（Residual Connection）和层归一化（Layer Normalization, LayerNorm### ）\n",
    "\n",
    "在 Transformer 架构中，**残差连接**（Residual Connection）与**层归一化**（LayerNorm）结合使用，统称为 **Add & Norm** 操作。\n",
    "\n",
    "### Add（残差连接，Residual Connection）\n",
    "\n",
    "> **ResNet**\n",
    "> Deep Residual Learning for Image Recognition | [arXiv 1512.03385](https://arxiv.org/pdf/1512.03385)\n",
    ">\n",
    "> **简单，但有效。**\n",
    "\n",
    "残差连接是一种跳跃连接（Skip Connection），它将层的输入直接加到输出上（观察架构图中的箭头），对应的公式如下：\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{SubLayer}(x) + x\n",
    "$$\n",
    "\n",
    "这种连接方式有效缓解了**深层神经网络的梯度消失**问题。\n",
    "\n",
    "#### Q: 为什么可以缓解梯度消失？\n",
    "\n",
    "首先，我们需要了解什么是梯度消失。\n",
    "\n",
    "在深度神经网络中，参数的梯度通过反向传播计算，其公式为：\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = \\frac{\\partial \\mathcal{L}}{\\partial h_n} \\cdot \\frac{\\partial h_n}{\\partial h_{n-1}} \\cdot \\ldots \\cdot \\frac{\\partial h_1}{\\partial W}\n",
    "$$\n",
    "当网络层数增加时，**链式法则**中的梯度相乘可能导致梯度值越来越小（梯度消失）或越来越大（梯度爆炸），使得模型难以训练和收敛。\n",
    "\n",
    "假设输出层的损失为 $\\mathcal{L}$，且 $\\text{SubLayer}(x)$ 表示为 $F(x)$。在没有残差连接的情况下，梯度通过链式法则计算为：\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial F(x)} \\cdot \\frac{\\partial F(x)}{\\partial x}\n",
    "$$\n",
    "如果 $\\frac{\\partial F(x)}{\\partial x}$ 的绝对值小于 1，那么随着层数的增加，梯度会呈快速缩小，导致梯度消失。\n",
    "\n",
    "引入残差连接后，输出变为 $F(x) + x$，其梯度为：\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial \\mathcal{L}}{\\partial (x + F(x))} \\cdot (1 + \\frac{\\partial F(x)}{\\partial x})\n",
    "$$\n",
    "这里，包含了一个常数项 1，这意味着即使 $\\frac{\\partial F(x)}{\\partial x}$ 很小，梯度仍然可以有效地反向传播，缓解梯度消失问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd90c58-ca5d-4d8a-9a99-e304ad9d5094",
   "metadata": {},
   "source": [
    "#### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db50649b-17c6-4524-bcdc-c6cae27538b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        \"\"\"\n",
    "        残差连接，用于在每个子层后添加残差连接和 Dropout。\n",
    "        \n",
    "        参数:\n",
    "            dropout: Dropout 概率，用于在残差连接前应用于子层输出，防止过拟合。\n",
    "        \"\"\"\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 残差连接的输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            sublayer: 子层模块的函数，多头注意力或前馈网络。\n",
    "\n",
    "        返回:\n",
    "            经过残差连接和 Dropout 处理后的张量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 将子层输出应用 dropout，然后与输入相加（参见论文 5.4 的表述或者本文「呈现」部分）\n",
    "        return x + self.dropout(sublayer(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8e21a7-dbe7-4277-93c4-8af067956eee",
   "metadata": {},
   "source": [
    "### Norm（层归一化，Layer Normalization）\n",
    "\n",
    "> Layer Normalization | [arXiv 1607.06450](https://arxiv.org/pdf/1607.06450)\n",
    "\n",
    "**层归一化**（LayerNorm）是一种归一化技术，用于提升训练的稳定性和模型的泛化能力。\n",
    "\n",
    "#### Q: BatchNorm 和 LayerNorm 的区别\n",
    "\n",
    "如果你听说过 **Batch Normalization (BatchNorm)**，或许会疑惑于二者的区别。\n",
    "\n",
    "假设输入张量的形状为 **(batch_size, feature_size)**，其中 `batch_size=32`，`feature_size=512`。\n",
    "\n",
    "- **batch_size**：表示批次中的样本数量。  \n",
    "- **feature_size**：表示每个样本的特征维度，即每个样本包含 512 个特征。\n",
    "\n",
    "这里的一行对应于一个样本，一列对应于一种特征属性。\n",
    "\n",
    "- BatchNorm 基于一个**批次**（batch）内的所有样本，针对**特征维度**（列）进行归一化，即在每一列（相同特征或嵌入维度上的 batch_size 个样本）上计算均值和方差。\n",
    "\n",
    "  - 对第 $j$ 列（特征）计算均值和方差：\n",
    "\n",
    "    $$\n",
    "    \\mu_j = \\frac{1}{\\text{batch\\_size}} \\sum_{i=1}^{\\text{batch\\_size}} x_{i,j}, \\quad \n",
    "    \\sigma^2_j = \\frac{1}{\\text{batch\\_size}} \\sum_{i=1}^{\\text{batch\\_size}} (x_{i,j} - \\mu_j)^2\n",
    "    $$\n",
    "\n",
    "- LayerNorm 基于**每个样本的所有特征**，针对**样本自身**（行内所有特征）进行归一化，即在每一行（一个样本的 embed_size 个特征）上计算均值和方差。\n",
    "\n",
    "  - 对第 $i$ 行（样本）计算均值和方差：\n",
    "\n",
    "    $$\n",
    "    \\mu_i = \\frac{1}{\\text{feature\\_size}} \\sum_{j=1}^{\\text{feature\\_size}} x_{i,j}, \\quad \n",
    "    \\sigma^2_i = \\frac{1}{\\text{feature\\_size}} \\sum_{j=1}^{\\text{feature\\_size}} (x_{i,j} - \\mu_i)^2\n",
    "    $$\n",
    "\n",
    "用表格说明：\n",
    "\n",
    "| 操作          | 处理维度                       | 解释                         |\n",
    "| ------------- | ------------------------------ | ---------------------------- |\n",
    "| **BatchNorm** | 对列（特征维度）归一化         | 每个特征在所有样本中的归一化 |\n",
    "| **LayerNorm** | 对行（样本内的特征维度）归一化 | 每个样本的所有特征一起归一化 |\n",
    "\n",
    "> BatchNorm 和 LayerNorm 在视频中也有讲解：[Transformer论文逐段精读【论文精读】25:40 - 32:04 部分](https://www.bilibili.com/video/BV1pu411o7BE/?share_source=copy_web&vd_source=e46571d631061853c8f9eead71bdb390&t=1540)，不过需要注意的是在 26:25 处应该除以的是标准差而非方差。\n",
    ">\n",
    "> ![BN vs LN](../assets/image-20241028172742399.png)\n",
    ">\n",
    "> 对于三维张量，比如图示的 (batch_size, seq_len, feature_size)，可以从立方体的左侧(batch_size, feature_size) 去看成二维张量进行切片。\n",
    "\n",
    "#### LayerNorm 的计算过程\n",
    "\n",
    "假设输入向量为 $x = (x_1, x_2, \\dots, x_d)$, LayerNorm 的计算步骤如下：\n",
    "\n",
    "1. **计算均值和方差**：\n",
    "   对输入的所有特征求均值 $\\mu$ 和方差 $\\sigma^2$：\n",
    "\n",
    "   $$\n",
    "   \\mu = \\frac{1}{d} \\sum_{j=1}^{d} x_j, \\quad \n",
    "   \\sigma^2 = \\frac{1}{d} \\sum_{j=1}^{d} (x_j - \\mu)^2\n",
    "   $$\n",
    "\n",
    "2. **归一化公式**：\n",
    "   将输入特征 $\\hat{x}_i$ 进行归一化：\n",
    "\n",
    "   $$\n",
    "   \\hat{x}_i = \\frac{x_i - \\mu}{\\sqrt{\\sigma^2 + \\epsilon}}\n",
    "   $$\n",
    "\n",
    "   其中, $\\epsilon$ 是一个很小的常数（比如 1e-9），用于防止除以零的情况。\n",
    "\n",
    "3. **引入可学习参数**：\n",
    "   归一化后的输出乘以 $\\gamma$ 并加上 $\\beta$, 公式如下：\n",
    "\n",
    "   $$\n",
    "   \\text{Output} = \\gamma \\hat{x} + \\beta\n",
    "   $$\n",
    "\n",
    "   其中 $\\gamma$ 和 $\\beta$ 是可学习的参数，用于进一步调整归一化后的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612252e5-64dc-4969-ac93-190648fa35c8",
   "metadata": {},
   "source": [
    "#### 代码实现\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f239e531-408b-4e1f-8b34-f03aaaf25005",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, feature_size, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        层归一化，用于对最后一个维度进行归一化。\n",
    "        \n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.gamma = nn.Parameter(torch.ones(feature_size))  # 可学习缩放参数，初始值为 1\n",
    "        self.beta = nn.Parameter(torch.zeros(feature_size))  # 可学习偏移参数，初始值为 0\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        std = x.std(dim=-1, keepdim=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184f530c-94a3-4734-b749-716c5dfa89ad",
   "metadata": {},
   "source": [
    "#### 澄清：LayerNorm 最后的缩放与线性层 (nn.Linear) 的区别\n",
    "\n",
    "见过线性层源码但不熟悉乘法运算符的同学可能会有一个错误的困惑：\n",
    "\n",
    "**最后不就是线性层的实现吗，为什么不直接用 `nn.Linear((x - mean) / (std + self.epsilon))` 实现呢？**\n",
    "\n",
    "乍一看，LayerNorm 的计算过程确实与 `nn.Linear` 有些相似：LayerNorm 对归一化后的输出进行了缩放（乘以 $\\gamma$）和偏移（加上 $\\beta$），但这两者的核心作用和参数运算方式存在**本质的不同**，接下来逐一澄清：\n",
    "\n",
    "1. `self.gamma * x` 实际上是逐元素缩放操作而非对输入做线性组合。\n",
    "\n",
    "2. self.gamma 的 shape 为 `(feature_size,)` 而非 `(feature_size, feature_size)`。\n",
    "\n",
    "3. 线性层的公式为: $\\text{Output} = x W^T + b$, 代码实现为：\n",
    "\n",
    "   ```python\n",
    "   # 初始化的 shape 是二维的\n",
    "   self.weight = nn.Parameter(torch.randn(out_features, in_features))  # 权重矩阵\n",
    "   self.bias = nn.Parameter(torch.zeros(out_features))  # 偏置向量\n",
    "   \n",
    "   # 计算\n",
    "   def forward(self, x):\n",
    "   \treturn torch.matmul(x, self.weight.T) + self.bias\n",
    "   ```\n",
    "\n",
    "LayerNorm 是 `* `逐元素乘积，nn.Linear 是 `torch.matmul()` 矩阵乘法，运行代码：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "# 创建两个张量 A 和 B\n",
    "A = torch.tensor([[1, 2], [3, 4]])  # 形状 (2, 2)\n",
    "B = torch.tensor([[5, 6], [7, 8]])  # 形状 (2, 2)\n",
    "\n",
    "### 1. 逐元素乘法\n",
    "elementwise_product = A * B  # 对应位置元素相乘\n",
    "print(\"逐元素乘法 (A * B) 的结果：\\n\", elementwise_product)\n",
    "\n",
    "### 2. 矩阵乘法\n",
    "matrix_product = torch.matmul(A, B)  # 矩阵乘法\n",
    "print(\"矩阵乘法 (torch.matmul(A, B)) 的结果：\\n\", matrix_product)\n",
    "\n",
    "```\n",
    "\n",
    "**输出**：\n",
    "\n",
    "```sql\n",
    "逐元素乘法 (A * B) 的结果：\n",
    " tensor([[ 5, 12],\n",
    "        [21, 32]])\n",
    "矩阵乘法 (torch.matmul(A, B)) 的结果：\n",
    " tensor([[19, 22],\n",
    "        [43, 50]])\n",
    "```\n",
    "\n",
    "可以看到二者并不是一个操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e00ce89-fd4f-4414-a77b-1adc5347fb95",
   "metadata": {},
   "source": [
    "### Add & Norm\n",
    "\n",
    "**操作步骤**：\n",
    "\n",
    "1. **残差连接**：将输入直接与输出相加。\n",
    "2. **层归一化**：对相加后的结果进行归一化。\n",
    "\n",
    "公式如下：\n",
    "\n",
    "$$\n",
    "\\text{Output} = \\text{LayerNorm}(x + \\text{SubLayer}(x))\n",
    "$$\n",
    "\n",
    "其中, $\\text{SubLayer}(x)$ 表示 Transformer 中的某个子层（如自注意力层或前馈网络层）的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5552be63-e7dd-46ad-af24-2ef8c4a4e43a",
   "metadata": {},
   "source": [
    "#### 代码实现\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a5c7381-1b0e-43c7-9a07-473651249a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        \"\"\"\n",
    "        子层连接，包括残差连接和层归一化，应用于 Transformer 的每个子层。\n",
    "\n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            dropout: 残差连接中的 Dropout 概率。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.residual = ResidualConnection(dropout)  # 使用 ResidualConnection 进行残差连接\n",
    "        self.norm = LayerNorm(feature_size, epsilon)  # 层归一化\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 将子层输出应用 dropout 后经过残差连接后再进行归一化，可见本文「呈现」部分\n",
    "        return self.norm(self.residual(x, sublayer))\n",
    "\n",
    "# 或者直接在 AddNorm 里面实现残差连接\n",
    "class SublayerConnection(nn.Module):\n",
    "    \"\"\"\n",
    "        子层连接的另一种实现方式，残差连接直接在该模块中实现。\n",
    "\n",
    "        参数:\n",
    "            feature_size: 输入特征的维度大小，即归一化的特征维度。\n",
    "            dropout: 残差连接中的 Dropout 概率。\n",
    "            epsilon: 防止除零的小常数。\n",
    "        \"\"\"\n",
    "    def __init__(self, feature_size, dropout=0.1, epsilon=1e-9):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(feature_size, epsilon)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # 将子层输出应用 dropout 后经过残差连接后再进行归一化，可见本文「呈现」部分\n",
    "        return self.norm(x + self.dropout(sublayer(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53ab0526-1b15-4197-bd82-5117743b69af",
   "metadata": {},
   "source": [
    "## 嵌入（Embeddings）\n",
    "\n",
    "> ![Embedding](../assets/image-20241029172114093.png)\n",
    "\n",
    "在 Transformer 模型中，**嵌入层**（Embedding Layer） 是处理输入和输出数据的关键步骤，因为模型实际操作的是**张量**（tensor），而非**字符串**（string）。在将输入文本传递给模型之前，首先需要进行**分词**（tokenization），即将文本拆解为多个 **token**，随后这些 token 会被映射为对应的 **token ID**，从而转换为模型可理解的数值形式。此时，数据的形状为 `(seq_len,)`，其中 `seq_len` 表示输入序列的长度。\n",
    "\n",
    "### Q: 为什么需要嵌入层？\n",
    "\n",
    "因为 token ID 只是整数标识符，彼此之间没有内在联系。如果直接使用这些整数，模型可能在训练过程中学习到一些模式，但无法充分捕捉词汇之间的语义关系，这显然不足以支撑起现在的大模型。\n",
    "\n",
    "举个简单的例子来理解“语义”关系：像“猫”和“狗”在向量空间中的表示应该非常接近，因为它们都是宠物；“男人”和“女人”之间的向量差异可能代表性别的区别。此外，不同语言的词汇，如“男人”（中文）和“man”（英文），如果在相同的嵌入空间中，它们的向量也会非常接近，反映出跨语言的语义相似性。同时，【“女人”和“woman”（中文-英文）】与【“男人”和“man”（中文-英文）】之间的差异也可能非常相似。\n",
    "\n",
    "对于模型而言，没有语义信息就像我们小时候刚开始读英语阅读报：“这些字母拼起来是什么？不知道。这些单词在说什么？不知道。”囫囵吞枣看完后去做题：“嗯，昨天对答案的时候，A 好像多一点，其他的差不多，那多选一点 A，其他平均分 :)。”\n",
    "\n",
    "所以，为了让模型捕捉到 token 背后复杂的语义（Semantic meaning）关系，我们需要将离散的 token ID 映射到一个高维的连续向量空间（Continuous, dense）。这意味着每个 token ID 会被转换为一个**嵌入向量**（embedding vector），期望通过这种方式让语义相近的词汇在向量空间中距离更近，使模型能更好地捕捉词汇之间的关系。当然，简单的映射无法做到这一点，因此需要“炼丹”——是的，嵌入层是可以训练的。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33fd5502-b383-411a-b0a4-748f7cd8be29",
   "metadata": {},
   "source": [
    "### 代码实现\n",
    "\n",
    "- **`nn.Embedding`**：创建嵌入层，将词汇表中的每个 token ID 映射为对应的嵌入向量。\n",
    "\n",
    "- **`vocab_size`**：词汇表的大小。\n",
    "\n",
    "- **`d_model`**：嵌入向量的维度大小。\n",
    "\n",
    "**特殊设计**\n",
    "\n",
    "> ![3.4](../assets/image-20241029173230358.png)\n",
    "\n",
    "- **缩放嵌入（Scaled Embedding）**：将嵌入层的输出（参数）乘以 $\\sqrt{d_{\\text{model}}}$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69f590b4-e76c-4c63-b602-c59d85e59027",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    \"\"\"\n",
    "    嵌入，将 token ID 转换为固定维度的嵌入向量，并进行缩放。\n",
    "\n",
    "    参数:\n",
    "        vocab_size: 词汇表大小。\n",
    "        d_model: 嵌入向量的维度。\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, d_model):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, d_model)\n",
    "        self.scale_factor = math.sqrt(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入张量，形状为 (batch_size, seq_len)，其中每个元素是 token ID。\n",
    "\n",
    "        返回:\n",
    "            缩放后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        return self.embed(x) * self.scale_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ae7918-0754-41ad-b1ee-6120af83cd5f",
   "metadata": {},
   "source": [
    "### Q: 什么是 nn.Embedding()？和 nn.Linear() 的区别是什么？\n",
    "\n",
    "其实非常简单，`nn.Embedding()` 就是从权重矩阵中查找与输入索引对应的行，类似于查找表操作，而 `nn.Linear()` 进行线性变换。直接对比二者的 `forward()` 方法：\n",
    "\n",
    "```python\n",
    "# Embedding\n",
    "def forward(self, input):\n",
    "\treturn self.weight[input]  # 没错，就是返回对应的行\n",
    "\n",
    "# Linear\n",
    "def forward(self, input):\n",
    "\ttorch.matmul(input, self.weight.T) + self.bias\n",
    "```\n",
    "\n",
    "运行下面的代码来验证：\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 设置随机种子\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# nn.Embedding() 权重矩阵形状为 (num_embeddings, embedding_dim)\n",
    "num_embeddings = 5  # 假设有 5 个 token\n",
    "embedding_dim = 3   # 每个 token 对应 3 维嵌入\n",
    "\n",
    "# 初始化嵌入层\n",
    "embedding = nn.Embedding(5, 3)\n",
    "\n",
    "# 整数索引\n",
    "input_indices = torch.tensor([0, 2, 4])\n",
    "\n",
    "# 查找嵌入\n",
    "output = embedding(input_indices)\n",
    "\n",
    "# 打印结果\n",
    "print(\"权重矩阵：\")\n",
    "print(embedding.weight.data)\n",
    "print(\"\\nEmbedding 输出：\")\n",
    "print(output)\n",
    "```\n",
    "\n",
    "**输出**：\n",
    "\n",
    "```sql\n",
    "权重矩阵：\n",
    "tensor([[ 0.3367,  0.1288,  0.2345],\n",
    "[ 0.2303, -1.1229, -0.1863],\n",
    "[ 2.2082, -0.6380,  0.4617],\n",
    "[ 0.2674,  0.5349,  0.8094],\n",
    "[ 1.1103, -1.6898, -0.9890]])\n",
    "\n",
    "Embedding 输出：\n",
    "tensor([[ 0.3367,  0.1288,  0.2345],\n",
    "[ 2.2082, -0.6380,  0.4617],\n",
    "[ 1.1103, -1.6898, -0.9890]], grad_fn=<EmbeddingBackward0>)\n",
    "```\n",
    "\n",
    "**要点**：\n",
    "\n",
    "- **权重矩阵**：嵌入层的权重矩阵，其形状为 `(num_embeddings, embedding_dim)`，熟悉线性层的同学可以理解为 `(in_features, out_features)`。\n",
    "- **Embedding 输出**：根据输入索引，从权重矩阵中提取对应的嵌入向量（行）。\n",
    "  - 在例子中，输入索引 `[0, 2, 4]`，因此输出了权重矩阵中第 0、2、4 行对应的嵌入向量。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38c1284-2bf4-471c-b868-f22131ecf3d7",
   "metadata": {},
   "source": [
    "## Softmax\n",
    "\n",
    "> ![Softmax](../assets/image-20241030161359558.png)\n",
    "\n",
    "在 Transformer 模型中，**Softmax** 函数不仅在计算**注意力权重**时用到，在预测阶段的输出处理环节也会用到，因为预测 token 的过程可以看成是**多分类问题**。\n",
    "\n",
    "**Softmax** 函数是一种常用的激活函数，能够将任意实数向量转换为**概率分布**，确保每个元素的取值范围在 [0, 1] 之间，并且所有元素的和为 1。其数学定义如下：\n",
    "$$\n",
    "\\text{Softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}}\n",
    "$$\n",
    "\n",
    "其中：\n",
    "\n",
    "- $x_i$ 表示输入向量中的第 $i$ 个元素。\n",
    "- $\\text{Softmax}(x_i)$ 表示输入 $x_i$ 转换后的概率。\n",
    "\n",
    "我们可以把 Softmax 看作一种**归一化的指数变换**。相比于简单的比例归一化 $\\frac{x_i}{\\sum_j x_j}$，Softmax 通过指数变换放大数值间的差异，让较大的值对应更高的概率，同时避免了负值和数值过小的问题。\n",
    "\n",
    "### 代码实现\n",
    "\n",
    "实际使用时可以直接调用 `nn.Softmax()`，这里手动实现一个简单的 Softmax 函数，并与 `nn.Softmax()` 的结果进行对比，以加深公式的印象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7dc53432-a55f-4224-9be6-6b413f1f80b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "根据公式实现的 Softmax 结果： tensor([0.0900, 0.2447, 0.6652])\n",
      "nn.Softmax 的结果： tensor([0.0900, 0.2447, 0.6652])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def softmax(x):\n",
    "    exp_x = torch.exp(x)\n",
    "    sum_exp_x = torch.sum(exp_x, dim=-1, keepdim=True)\n",
    "    return exp_x / sum_exp_x\n",
    "\n",
    "# 测试向量\n",
    "x = torch.tensor([1.0, 2.0, 3.0])\n",
    "\n",
    "# 根据公式实现的 Softmax\n",
    "result = softmax(x)\n",
    "\n",
    "# 使用 nn.Softmax\n",
    "softmax = nn.Softmax(dim=-1)\n",
    "nn_result = softmax(x)\n",
    "\n",
    "print(\"根据公式实现的 Softmax 结果：\", result)\n",
    "print(\"nn.Softmax 的结果：\", nn_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fdb71e-1720-403b-b902-2b5b56dfd2b8",
   "metadata": {},
   "source": [
    "## 位置编码（Positional Encoding）\n",
    "\n",
    "> ![Positional Encoding](../assets/image-20241030195425650.png)\n",
    "\n",
    "在 Transformer 模型中，由于不是循环（RNN）结构，模型本身无法捕捉输入序列中元素的位置信息。回顾一下注意力机制的计算过程，**得分**（score）是通过查询向量（query）和键向量（key）之间的内积得到的，生成的注意力权重（attention weights）也只是基于这些内积结果，这个操作不会捕捉到位置信息。\n",
    "\n",
    "举个例子，把序列 `[\"A\", \"B\", \"C\"]` 改成 `[\"B\", \"A\", \"C\"]`，得到的输出也会是原来的结果按同样顺序打乱后的形式，假设原输出为 `[Z_A, Z_B, Z_C]`，打乱后的输出将变为 `[Z_B, Z_A, Z_C]`。\n",
    "\n",
    "所以如果嵌入向量本身不包含位置信息，就意味着**输入元素的顺序不会影响输出的权重计算，模型无法从中捕捉到序列的顺序信息**，换句话说，只是输出的位置跟着对应变化，但对应的计算结果不会改变，可以用一句诗概括当前的现象：「天涯若比邻」。\n",
    "\n",
    "为了解决这个问题，Transformer 引入了**位置编码（Positional Encoding）**：为每个位置生成一个向量，这个向量与对应的嵌入向量相加，从而在输入中嵌入位置信息。\n",
    "\n",
    "在原始论文中，Transformer 使用的是固定位置编码（Positional Encoding），其公式如下：\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "PE_{(pos, 2i)} &= \\sin\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right), \\\\\n",
    "PE_{(pos, 2i+1)} &= \\cos\\left(\\frac{pos}{10000^{2i/d_{\\text{model}}}}\\right).\n",
    "\\end{aligned}\n",
    "$$\n",
    "其中：\n",
    "\n",
    "- $pos$ 表示位置索引（Position）。\n",
    "- $i$ 表示维度索引。\n",
    "- $d_{\\text{model}}$ 是嵌入向量的维度。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a61ab0-9724-4abd-b955-a8c3cc8d8862",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cde54ab0-5410-42ef-bcb5-3a49fc6c95db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        \"\"\"\n",
    "        位置编码，为输入序列中的每个位置添加唯一的位置表示，以引入位置信息。\n",
    "\n",
    "        参数:\n",
    "            d_model: 嵌入维度，即每个位置的编码向量的维度。\n",
    "            dropout: 位置编码后应用的 Dropout 概率。\n",
    "            max_len: 位置编码的最大长度，适应不同长度的输入序列。\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)  # 正如论文 5.4 节所提到的，需要将 Dropout 应用在 embedding 和 positional encoding 相加的时候\n",
    "        \n",
    "        # 创建位置编码矩阵，形状为 (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len).unsqueeze(1)  # 位置索引 (max_len, 1)\n",
    "        \n",
    "        # 计算每个维度对应的频率\n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        # 将位置和频率结合，计算 sin 和 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)  # 偶数维度\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)  # 奇数维度\n",
    "        \n",
    "        # 增加一个维度，方便后续与输入相加，形状变为 (1, max_len, d_model)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # 将位置编码注册为模型的缓冲区，不作为参数更新\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入序列的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "\n",
    "        返回:\n",
    "            加入位置编码和 Dropout 后的嵌入向量，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        # 取出与输入序列长度相同的部分位置编码，并与输入相加\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        \n",
    "        # 应用 dropout\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9500472-ff55-40bc-892f-6d46e0ea83f5",
   "metadata": {},
   "source": [
    "### 可视化\n",
    "\n",
    "位置编码在维度 4、5、6 和 7 上的变化：\n",
    "\n",
    "![positional_encoding](../assets/positional_encoding.png)\n",
    "\n",
    "你可以使用下面的代码来可视化其他的维度并保存图片到本地。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b8fe67-cea2-462b-97e1-15f55cab3ed2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def visualize_positional_encoding():\n",
    "    # 初始化位置编码类，设置嵌入向量维度为20，dropout 概率为0，最大序列长度为100\n",
    "    pe = PositionalEncoding(20, dropout=0, max_len=100)\n",
    "    # 使用全零张量进行前向传播，形状为(1, 100, 20)，这样返回的值就是位置编码\n",
    "    y = pe.forward(torch.zeros(1, 100, 20))\n",
    "\n",
    "    # 设置图形大小为 12x6\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # 可视化第 4 到第 7 维的编码值\n",
    "    for dim in [4, 5, 6, 7]:\n",
    "        plt.plot(range(100), y[0, :, dim].numpy(), label=f'Dimension {dim}')\n",
    "    \n",
    "    # 设置图形标题、坐标轴标签和网格\n",
    "    plt.title(\"Positional Encoding Visualization\")\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Encoding Value\")\n",
    "    plt.legend(title=\"Dimensions\")\n",
    "    plt.grid(True)\n",
    "\n",
    "    # 将图像保存为 PNG 格式\n",
    "    plt.savefig(\"positional_encoding.png\", format=\"png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "# 可视化位置编码并进行保存\n",
    "visualize_positional_encoding()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc35c72-f843-4460-bf12-abf74b9154c9",
   "metadata": {},
   "source": [
    "## 输入处理\n",
    "\n",
    "> ![image-20241102111054720](../assets/image-20241102111054720.png)\n",
    "\n",
    "在完成嵌入和位置编码的代码后，就可以实现编码器和解码器的输入处理。二者处理代码的主体完全一致，只是 `vocab_size` 根据实际情况可能会有所不同。\n",
    "\n",
    "### 编码器输入处理\n",
    "\n",
    "> ![image-20241102111130473](../assets/image-20241102111130473.png)\n",
    "\n",
    "编码器的输入由输入嵌入（Input Embedding）和位置编码（Positional Encoding）组成，在机器翻译任务中，还可以称为源语言嵌入（Source Embedding）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dfc9afb7-1958-4f82-97fb-257db16bcf98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SourceEmbedding(nn.Module):\n",
    "    def __init__(self, src_vocab_size, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        源序列嵌入，将输入的 token 序列转换为嵌入向量并添加位置编码。\n",
    "\n",
    "        参数:\n",
    "            src_vocab_size: 源语言词汇表的大小\n",
    "            d_model: 嵌入向量的维度\n",
    "            dropout: 在位置编码后应用的 Dropout 概率\n",
    "        \"\"\"\n",
    "        super(SourceEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(src_vocab_size, d_model)  # 词嵌入层\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)  # 位置编码层\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 源语言序列的输入张量，形状为 (batch_size, seq_len_src)，其中每个元素是 token ID。\n",
    "\n",
    "        返回:\n",
    "            添加位置编码后的嵌入向量，形状为 (batch_size, seq_len_src, d_model)。\n",
    "        \"\"\"\n",
    "        x = self.embed(x)  # 生成词嵌入 (batch_size, seq_len_src, d_model)\n",
    "        return self.positional_encoding(x)  # 加入位置编码\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f430e07a-39d1-4598-ad76-082c8a579ca1",
   "metadata": {},
   "source": [
    "### 解码器输入处理\n",
    "\n",
    "> ![image-20241102111231882](../assets/image-20241102111231882.png)\n",
    "\n",
    "解码器的输入由输出嵌入（Output Embedding）和位置编码（Positional Encoding）组成，在机器翻译这个任务中也可以称为目标语言嵌入（Target Embedding），为了避免与最终输出混淆，使用 `TargetEmbedding` 进行实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9640d21a-beb1-4811-981c-948932881dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TargetEmbedding(nn.Module):\n",
    "    def __init__(self, tgt_vocab_size, d_model, dropout=0.1):\n",
    "        \"\"\"\n",
    "        目标序列嵌入，将目标序列的 token ID 转换为嵌入向量并添加位置编码。\n",
    "\n",
    "        参数:\n",
    "            tgt_vocab_size: 目标语言词汇表的大小\n",
    "            d_model: 嵌入向量的维度\n",
    "            dropout: 在位置编码后应用的 Dropout 概率\n",
    "        \"\"\"\n",
    "        super(TargetEmbedding, self).__init__()\n",
    "        self.embed = Embeddings(tgt_vocab_size, d_model)  # 词嵌入层\n",
    "        self.positional_encoding = PositionalEncoding(d_model, dropout)  # 位置编码层\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 目标序列的输入张量，形状为 (batch_size, seq_len_tgt)，其中每个元素是 token ID。\n",
    "\n",
    "        返回:\n",
    "            添加位置编码后的嵌入向量，形状为 (batch_size, seq_len_tgt, d_model)。\n",
    "        \"\"\"\n",
    "        x = self.embed(x)  # 生成词嵌入 (batch_size, seq_len_tgt, d_model)\n",
    "        return self.positional_encoding(x)  # 加入位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d918e4fa-9649-4c45-8c7e-1a20a8ed6177",
   "metadata": {},
   "source": [
    "## 掩码\n",
    "\n",
    "在 Transformer 模型中，掩码用于控制注意力机制中哪些位置需要被忽略，本文在之前讲解过为什么需要掩码机制，在这里我们将分别实现它们。\n",
    "\n",
    "### 填充掩码（Padding Mask）\n",
    "\n",
    "填充掩码用于在注意力计算时屏蔽填充 `<PAD>` 位置，防止模型计算注意力权重的时候考虑这些无意义的位置，在编码器的自注意力中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c006a806-c725-4e02-a35f-cdf30a1aee05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token=0):\n",
    "    # seq 的形状为 (batch_size, seq_len)\n",
    "    mask = (seq != pad_token).unsqueeze(1).unsqueeze(2)  # (batch_size, 1, 1, seq_len)\n",
    "    return mask  # 在注意力计算时，填充值为 0 的位置会被屏蔽"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c35b559-8842-4d49-aac5-1e330fc8a35e",
   "metadata": {},
   "source": [
    "**注意**：这里接受的参数为 pad_token_id，这意味着掩码操作在嵌入操作前，也就是分词（tokenize）然后映射为 Token IDs 后进行。\n",
    "\n",
    "#### 示例\n",
    "\n",
    "假设我们有以下两个序列，经过分词和映射后："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cedfd9b-9375-496a-8ae8-3153edd3e925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True,  True,  True, False, False]]],\n",
      "\n",
      "\n",
      "        [[[ True,  True, False, False, False]]]])\n"
     ]
    }
   ],
   "source": [
    "seq = torch.tensor([[5, 7, 9, 0, 0], [8, 6, 0, 0, 0]])  # 0 表示 <PAD>\n",
    "print(create_padding_mask(seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3245d9-e002-4ac6-ac1c-10416529238d",
   "metadata": {},
   "source": [
    "### 未来信息掩码（Look-ahead Mask）\n",
    "\n",
    "未来信息掩码用于在解码器中屏蔽未来的位置，防止模型在预测下一个词时“偷看”答案（训练时），在解码器中使用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "04194914-47dc-494b-a6cd-c09adde9f234",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    mask = torch.tril(torch.ones(size, size)).type(torch.bool)  # 下三角矩阵\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274ccb0-a293-48f1-8017-fde1cb96d0d9",
   "metadata": {},
   "source": [
    "#### 示例\n",
    "\n",
    "对于序列长度 5："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "736f8fbe-b8ac-4cfe-a99f-3c0f7d60bc22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ True, False, False, False, False],\n",
      "        [ True,  True, False, False, False],\n",
      "        [ True,  True,  True, False, False],\n",
      "        [ True,  True,  True,  True, False],\n",
      "        [ True,  True,  True,  True,  True]])\n"
     ]
    }
   ],
   "source": [
    "print(create_look_ahead_mask(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "167a5e22-909e-4854-9b02-769fda248efb",
   "metadata": {},
   "source": [
    "### 组合掩码\n",
    "\n",
    "在实际应用中，我们需要将填充掩码和未来信息掩码进行组合，以同时实现两种掩码的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a353df7c-afe3-43a3-b646-21f86d7e5cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_decoder_mask(tgt_seq, pad_token=0):\n",
    "    padding_mask = create_padding_mask(tgt_seq, pad_token)  # (batch_size, 1, 1, seq_len_tgt)\n",
    "    look_ahead_mask = create_look_ahead_mask(tgt_seq.size(1)).to(tgt_seq.device)  # (seq_len_tgt, seq_len_tgt)\n",
    "\n",
    "    combined_mask = look_ahead_mask.unsqueeze(0) & padding_mask  # (batch_size, 1, seq_len_tgt, seq_len_tgt)\n",
    "    return combined_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db08f889-3179-4b17-8094-8fcf186a83cd",
   "metadata": {},
   "source": [
    "#### 示例\n",
    "\n",
    "假设目标序列 `tgt_seq` 为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18818e34-35e3-4f74-9c8e-3c1655b1927c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[ True, False, False, False, False],\n",
      "          [ True,  True, False, False, False],\n",
      "          [ True,  True,  True, False, False],\n",
      "          [ True,  True,  True,  True, False],\n",
      "          [ True,  True,  True,  True, False]]]])\n"
     ]
    }
   ],
   "source": [
    "tgt_seq = torch.tensor([[1, 2, 3, 4, 0]])  # 0 表示 <PAD>\n",
    "print(create_decoder_mask(tgt_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66634a2a-e295-4603-80ae-92cc0958b8b1",
   "metadata": {},
   "source": [
    "# 子层模块\n",
    "\n",
    "## 编码器层 （Encoder Layer）\n",
    "\n",
    "> ![Encoder](../assets/image-20241028204711949.png)\n",
    "\n",
    "**组件**：\n",
    "\n",
    "- 多头自注意力（Multi-Head Self-Attention）\n",
    "- 前馈神经网络（Feed Forward）\n",
    "- 残差连接和层归一化（Add & Norm），或称之为子层连接（SublayerConnection）\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff78c62f-c857-43fe-be23-85e8487795a3",
   "metadata": {},
   "source": [
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "579aeb66-271b-4593-bcfa-84abbe33d87b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        编码器层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h)  # 多头自注意力（Multi-Head Self-Attention）\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # 前馈神经网络\n",
    "        \n",
    "        # 定义两个子层连接，分别用于多头自注意力和前馈神经网络（对应模型架构图中的两个残差连接）\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(2)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            x: 输入张量，形状为 (batch_size, seq_len, d_model)。\n",
    "            src_mask: 源序列掩码，用于自注意力。\n",
    "\n",
    "        返回:\n",
    "            编码器层的输出，形状为 (batch_size, seq_len, d_model)。\n",
    "        \"\"\"\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, src_mask))  # 自注意力子层\n",
    "        x = self.sublayers[1](x, self.feed_forward)  # 前馈子层\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c429535-d2e8-4b5c-b5f6-3a46ce5b4771",
   "metadata": {},
   "source": [
    "## 解码器层（Decoder Layer）\n",
    "\n",
    "> ![Decoder](../assets/image-20241101224129307.png)\n",
    "\n",
    "**组件**：\n",
    "\n",
    "- 掩码多头自注意力（Masked Multi-Head Self-Attention）\n",
    "- 多头交叉注意力（Multi-Head Cross-Attention）\n",
    "- 前馈神经网络（Feed Forward）\n",
    "- 残差连接和归一化（Add & Norm），或称之为子层连接（SublayerConnection）\n",
    "\n",
    "### 代码实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fc67f7f-f36b-4929-9417-c20825c159fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        解码器层。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h)  # 掩码多头自注意力（Masked Multi-Head Self-Attention）\n",
    "        self.cross_attn = MultiHeadAttention(d_model, h)  # 多头交叉注意力（Multi-Head Cross-Attention）\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)  # 前馈神经网络\n",
    "        \n",
    "        # 定义三个子层连接，分别用于掩码多头自注意力、多头交叉注意力和前馈神经网络（对应模型架构图中的三个残差连接）\n",
    "        self.sublayers = nn.ModuleList([SublayerConnection(d_model, dropout) for _ in range(3)])\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        参数:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 源序列掩码，用于交叉注意力\n",
    "            tgt_mask: 目标序列掩码，用于自注意力\n",
    "        返回:\n",
    "            x: 解码器层的输出\n",
    "        \"\"\"\n",
    "        # 第一个子层：掩码多头自注意力（Masked Multi-Head Self-Attention）\n",
    "        x = self.sublayers[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        \n",
    "        # 第二个子层：交叉多头注意力（Multi-Head Cross-Attention），使用编码器的输出 memory\n",
    "        x = self.sublayers[1](x, lambda x: self.cross_attn(x, memory, memory, src_mask))\n",
    "        \n",
    "        # 第三个子层：前馈神经网络\n",
    "        x = self.sublayers[2](x, self.feed_forward)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec07ef1-0a6e-442b-9d30-a7d0d98df84e",
   "metadata": {},
   "source": [
    "## 编码器（Encoder）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c400b455-cab7-4b27-a023-25f4348b0407",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, N, h, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        编码器，由 N 个 EncoderLayer 堆叠而成。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            N: 编码器层的数量\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, h, d_ff, dropout) for _ in range(N)\n",
    "        ])\n",
    "        self.norm = LayerNorm(d_model)  # 最后层归一化\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 输入张量 (batch_size, seq_len, d_model)\n",
    "            mask: 输入掩码\n",
    "        \n",
    "        返回:\n",
    "            编码器的输出\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)  # 最后层归一化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3f8ff6-7e40-4424-8941-2431efa99e6e",
   "metadata": {},
   "source": [
    "## 解码器（Decoder）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ad821e6f-9a46-46e8-9bb2-45b4585cf157",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, N, h, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        解码器，由 N 个 DecoderLayer 堆叠而成。\n",
    "        \n",
    "        参数:\n",
    "            d_model: 嵌入维度\n",
    "            N: 解码器层的数量\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, h, d_ff, dropout) for _ in range(N)\n",
    "        ])\n",
    "        self.norm = LayerNorm(d_model)  # 最后层归一化\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "        \n",
    "        参数:\n",
    "            x: 解码器输入 (batch_size, seq_len_tgt, d_model)\n",
    "            memory: 编码器的输出 (batch_size, seq_len_src, d_model)\n",
    "            src_mask: 用于交叉注意力的源序列掩码\n",
    "            tgt_mask: 用于自注意力的目标序列掩码\n",
    "            \n",
    "        返回:\n",
    "            解码器的输出\n",
    "        \"\"\"\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)  # 最后层归一化\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9935f179-cb92-4260-ba68-047e7d66ce99",
   "metadata": {},
   "source": [
    "# 完整模型\n",
    "\n",
    "> ![模型架构图](../assets/20241023202539.png)\n",
    "\n",
    "**完整组件**：\n",
    "\n",
    "- **输入嵌入和位置编码**：\n",
    "  - `SourceEmbedding`：对源序列进行嵌入并添加位置编码。\n",
    "  - `TargetEmbedding`：对目标序列进行嵌入并添加位置编码。\n",
    "\n",
    "- **多头注意力和前馈网络**：\n",
    "\n",
    "  - `MultiHeadAttention`：多头注意力机制。\n",
    "  - `PositionwiseFeedForward`：位置前馈网络。\n",
    "\n",
    "- **编码器和解码器**：\n",
    "\n",
    "  - `Encoder`：由多个 `EncoderLayer` 堆叠而成。\n",
    "  - `Decoder`：由多个 `DecoderLayer` 堆叠而成。\n",
    "\n",
    "- **输出层**：\n",
    "\n",
    "  - `fc_out`：线性层，将解码器的输出映射到目标词汇表维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2a571327-9da0-41e3-9abb-2be3e49e07a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, N, h, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Transformer 模型，由编码器和解码器组成。\n",
    "\n",
    "        参数:\n",
    "            src_vocab_size: 源语言词汇表大小\n",
    "            tgt_vocab_size: 目标语言词汇表大小\n",
    "            d_model: 嵌入维度\n",
    "            N: 编码器和解码器的层数\n",
    "            h: 多头注意力的头数\n",
    "            d_ff: 前馈神经网络的隐藏层维度\n",
    "            dropout: Dropout 概率\n",
    "        \"\"\"\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        # 输入嵌入和位置编码，src 对应于编码器输入，tgt 对应于解码器输入\n",
    "        self.src_embedding = SourceEmbedding(src_vocab_size, d_model, dropout)\n",
    "        self.tgt_embedding = TargetEmbedding(tgt_vocab_size, d_model, dropout)  # 共享：self.tgt_embedding = self.src_embedding\n",
    "\n",
    "        # 编码器和解码器\n",
    "        self.encoder = Encoder(d_model, N, h, d_ff, dropout)\n",
    "        self.decoder = Decoder(d_model, N, h, d_ff, dropout)\n",
    "\n",
    "        # 输出线性层\n",
    "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        \"\"\"\n",
    "        前向传播函数。\n",
    "\n",
    "        参数:\n",
    "            src: 源序列输入 (batch_size, seq_len_src)\n",
    "            tgt: 目标序列输入 (batch_size, seq_len_tgt)\n",
    "\n",
    "        返回:\n",
    "            Transformer 的输出（未经过 Softmax）\n",
    "        \"\"\"\n",
    "        # 生成掩码\n",
    "        src_mask = create_padding_mask(src)\n",
    "        tgt_mask = create_decoder_mask(tgt)\n",
    "\n",
    "        # 编码器\n",
    "        enc_output = self.encoder(self.src_embedding(src), src_mask)\n",
    "\n",
    "        # 解码器\n",
    "        dec_output = self.decoder(self.tgt_embedding(tgt), enc_output, src_mask, tgt_mask)\n",
    "\n",
    "        # 输出层\n",
    "        output = self.fc_out(dec_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd886389-5d69-4a47-9306-6555c563fdf7",
   "metadata": {},
   "source": [
    "### 实例化\n",
    "\n",
    "使用 Transformer base 的参数配置来实例化模型并打印模型架构："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "79c88ba8-255f-4343-b52f-64872bafe353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (src_embedding): SourceEmbedding(\n",
      "    (embed): Embeddings(\n",
      "      (embed): Embedding(5000, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (tgt_embedding): TargetEmbedding(\n",
      "    (embed): Embeddings(\n",
      "      (embed): Embedding(5000, 512)\n",
      "    )\n",
      "    (positional_encoding): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "  )\n",
      "  (encoder): Encoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x EncoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (sublayers): ModuleList(\n",
      "          (0-1): 2 x SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x DecoderLayer(\n",
      "        (self_attn): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (cross_attn): MultiHeadAttention(\n",
      "          (w_q): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_k): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (w_v): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (fc_out): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        )\n",
      "        (sublayers): ModuleList(\n",
      "          (0-2): 3 x SublayerConnection(\n",
      "            (norm): LayerNorm()\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm()\n",
      "  )\n",
      "  (fc_out): Linear(in_features=512, out_features=5000, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 定义词汇表大小（根据数据集）\n",
    "src_vocab_size = 5000  # 源语言词汇表大小\n",
    "tgt_vocab_size = 5000  # 目标语言词汇表大小\n",
    "\n",
    "# 使用 Transformer base 参数\n",
    "d_model = 512      # 嵌入维度\n",
    "N = 6              # 编码器和解码器的层数\n",
    "h = 8              # 多头注意力的头数\n",
    "d_ff = 2048        # 前馈神经网络的隐藏层维度\n",
    "dropout = 0.1      # Dropout 概率\n",
    "\n",
    "# 实例化模型\n",
    "model = Transformer(\n",
    "    src_vocab_size=src_vocab_size,\n",
    "    tgt_vocab_size=tgt_vocab_size,\n",
    "    d_model=d_model,\n",
    "    N=N,\n",
    "    h=h,\n",
    "    d_ff=d_ff,\n",
    "    dropout=dropout\n",
    ")\n",
    "\n",
    "# 打印模型架构\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfb72d4-32ad-4aca-99de-db8150a707b4",
   "metadata": {},
   "source": [
    "### 示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0bc7d783-0170-495f-8b9e-6544ec5ad4a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source embedding shape: torch.Size([32, 10, 512])\n",
      "Encoder output shape: torch.Size([32, 10, 512])\n",
      "Target embedding shape: torch.Size([32, 15, 512])\n",
      "Decoder output shape: torch.Size([32, 15, 512])\n",
      "Final output shape: torch.Size([32, 15, 5000])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 假设\n",
    "batch_size = 32\n",
    "seq_len_src = 10\n",
    "seq_len_tgt = 15\n",
    "\n",
    "# 构造输入\n",
    "src = torch.randint(0, 100, (batch_size, seq_len_src))  # (batch_size, seq_len_src)\n",
    "tgt = torch.randint(0, 100, (batch_size, seq_len_tgt))  # (batch_size, seq_len_tgt)\n",
    "\n",
    "# 获取掩码用于打印编码器和解码器的输出\n",
    "src_mask = create_padding_mask(src)\n",
    "tgt_mask = create_decoder_mask(tgt)\n",
    "\n",
    "# 模型最终输出\n",
    "output = model(src, tgt)\n",
    "\n",
    "# 打印各部分的输出形状\n",
    "print(\"Source embedding shape:\", model.src_embedding(src).shape)  # (batch_size, seq_len_src, d_model)\n",
    "print(\"Encoder output shape:\", model.encoder(model.src_embedding(src), src_mask).shape)  # (batch_size, seq_len_src, d_model)\n",
    "print(\"Target embedding shape:\", model.tgt_embedding(tgt).shape)  # (batch_size, seq_len_tgt, d_model)\n",
    "print(\"Decoder output shape:\", model.decoder(model.tgt_embedding(tgt), model.encoder(model.src_embedding(src), src_mask), src_mask, tgt_mask).shape)  # (batch_size, seq_len_tgt, d_model)\n",
    "print(\"Final output shape:\", output.shape)  # (batch_size, seq_len_tgt, tgt_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92b3c1-072d-4ab7-ab86-8ac139a3825e",
   "metadata": {},
   "source": [
    "### 对比 PyTorch 官方实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "167f90cc-4578-4a84-8b04-bdb8cd3374f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-5): 6 x TransformerDecoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (multihead_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "        (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "        (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.1, inplace=False)\n",
      "        (dropout2): Dropout(p=0.1, inplace=False)\n",
      "        (dropout3): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# 使用 Transformer base 参数\n",
    "d_model = 512      # 嵌入维度\n",
    "N = 6              # 编码器和解码器的层数\n",
    "h = 8              # 多头注意力的头数\n",
    "d_ff = 2048        # 前馈神经网络的隐藏层维度\n",
    "dropout = 0.1      # Dropout 概率\n",
    "\n",
    "model = nn.Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=h,\n",
    "    num_encoder_layers=N,\n",
    "    num_decoder_layers=N,\n",
    "    dim_feedforward=d_ff,\n",
    "    dropout=dropout,\n",
    "    batch_first=True\n",
    ")\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb4af80-a2a2-472f-9f0b-4007c2a02e80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
